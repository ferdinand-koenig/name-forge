{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NameForge: AI Domain Name Generator Homework\n",
    "\n",
    "**Author:** Ferdinand Koenig\n",
    "**Date:** Sep 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook documents the AI Engineer homework assignment for building a domain name generator.\n",
    "Objectives:\n",
    "\n",
    "- Generate synthetic dataset of business descriptions → domain names\n",
    "- Build a baseline domain generator (mock / open-source LLM)\n",
    "- Implement evaluation framework (LLM-as-a-judge / safety checks)\n",
    "- Analyze edge cases and iteratively improve\n",
    "- Ensure safety guardrails for inappropriate content\n"
   ],
   "id": "5cc073c8f706f17e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T09:15:24.683998Z",
     "start_time": "2025-09-05T09:15:24.212188Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "bd48274984db08b6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1️⃣ Step: Synthetic Dataset Creation\n",
    "\n",
    "### 1.1 Methodology\n",
    "\n",
    "**Objective:**\n",
    "Generate a synthetic dataset of business descriptions mapped to domain names, with diversity in business types, complexity levels, and edge cases, while ensuring safety and reproducibility.\n",
    "\n",
    "**Steps Taken:**\n",
    "\n",
    "1. **Vocabulary Selection**\n",
    "   - **Business types:** cafe, restaurant, tech startup, online store, boutique, law firm, travel agency, bookstore, etc.\n",
    "   - **Adjectives / descriptors:** organic, eco-friendly, bright, cozy, modern, smart, fresh, premium, global, innovative\n",
    "   - **Nouns / themes:** hub, shop, store, lab, studio, solutions, works, spot, corner\n",
    "   - **TLDs:** .com, .net, .org, .io, .co\n",
    "   - Vocabulary is stored externally in `src/vocab.py` for maintainability and easy updates.\n",
    "\n",
    "2. **Complexity Levels**\n",
    "   - **Simple:** Short, straightforward descriptions → short domains (e.g., “Organic cafe”)\n",
    "   - **Medium:** Include location or moderate complexity (e.g., “Organic cafe in downtown area”)\n",
    "   - **Complex:** Long or multi-purpose descriptions (e.g., “Premium organic cafe in downtown area offering community events for busy professionals”)\n",
    "\n",
    "3. **Edge Cases**\n",
    "   - ~5% of entries include unusual or extreme cases:\n",
    "     - Extremely long business descriptions\n",
    "     - Very short or ambiguous descriptions\n",
    "     - Uncommon characters (e.g., symbols @$%^)\n",
    "   - These cases test model robustness and evaluation coverage.\n",
    "\n",
    "4. **Safety Guardrails**\n",
    "   - Forbidden words: `adult`, `nude`, `porn`, `illegal`\n",
    "   - Any generated domain containing forbidden words is replaced with `blocked.com`\n",
    "\n",
    "5. **Dataset Generation Process**\n",
    "   - Randomly combine adjectives, nouns, and business types according to complexity\n",
    "   - Assign complexity distribution: 40% simple, 40% medium, 20% complex\n",
    "   - Randomly insert edge cases\n",
    "   - Save datasets as CSV with fields: `business_description`, `domain_name`, `complexity`\n",
    "\n",
    "6. **Train/Test Split**\n",
    "   - Train dataset: 500 entries\n",
    "   - Test dataset: 100 entries\n",
    "   - Stored in `data/raw/` as `train_dataset.csv` and `test_dataset.csv`\n",
    "\n",
    "### 1.2 Practical Considerations\n",
    "\n",
    "- **Reflecting client needs:**\n",
    "  - The synthetic dataset is designed to mimic the examples provided in the homework task, ensuring generated domain names are relevant to realistic business descriptions.\n",
    "\n",
    "- **Resource efficiency:**\n",
    "  - No fine-tuned LLM is used at this stage due to GPU requirements.\n",
    "  - No external API calls (OpenAI, Claude, etc.) are used because free accounts may not have access or sufficient credits.\n",
    "\n",
    "- **Reproducibility:**\n",
    "  - Dataset generation relies on deterministic Python code with controlled randomness (`random` module).\n",
    "  - Vocabulary is externalized for maintainability (`src/vocab.py`).\n",
    "  - The process can be fully run on a standard laptop without specialized hardware.\n",
    "\n",
    "- **Edge cases and safety:**\n",
    "  - ~5% of entries are extreme or unusual to test model robustness.\n",
    "  - Safety guardrails ensure forbidden words are blocked.\n",
    "\n"
   ],
   "id": "1dd0965a5267c931"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:09:20.116242Z",
     "start_time": "2025-09-06T11:09:20.034994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data_utils import generate_train_test\n",
    "\n",
    "generate_train_test(train_size=3_000, test_size=100)"
   ],
   "id": "718228688be801a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 3000 entries\n",
      "Dataset generated at data/raw/train_dataset.csv with 3000 entries\n",
      "generating 100 entries\n",
      "Dataset generated at data/raw/test_dataset.csv with 100 entries\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LoRA Fine-Tuning of Mistral-7B-Instruct (4-bit)\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "We performed **parameter-efficient fine-tuning** of a large language model (Mistral-7B-Instruct) using **LoRA (Low-Rank Adaptation)** in combination with **4-bit quantization** and Hugging Face’s Trainer.\n",
    "\n",
    "- **Dataset:** CSV of business descriptions → domain names\n",
    "- **Prompting:** YAML template for structured input\n",
    "- **Target:** Fine-tune the model for domain-specific naming suggestions\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why LoRA?\n",
    "\n",
    "- Traditional full fine-tuning of a 7B-parameter model is **compute- and memory-intensive**.\n",
    "- LoRA introduces **small trainable adapters** into existing weights:\n",
    "  - Adds low-rank matrices `A` and `B` to frozen weights:\n",
    "    \\[\n",
    "    W' = W + BA\n",
    "    \\]\n",
    "  - `W` is frozen (pre-trained weight)\n",
    "  - `BA` is trainable, tiny (~0.1% of total params)\n",
    "- **Advantages:**\n",
    "  - **Memory-efficient**: Only a few million parameters trainable instead of billions.\n",
    "  - **Fast training**: Lightweight updates.\n",
    "  - **Task-effective**: Adapts attention patterns without altering most of the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why 4-bit Quantization?\n",
    "\n",
    "- Reduces VRAM usage for large models (7B parameters) from ~28GB → 4–5GB for inference/training.\n",
    "- Makes fine-tuning feasible on a single GPU (48 GB in our case) while keeping speed reasonable.\n",
    "- Works well with LoRA because **most parameters remain frozen**, so low-precision weights are sufficient.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. GPU Capacity and Utilization\n",
    "\n",
    "- **GPU:** 48 GB VRAM\n",
    "- **Current usage:** 4–5 GB VRAM, ~40% compute utilization\n",
    "- **Why low?**\n",
    "  - LoRA trains only 6.8M parameters out of 7.25B → tiny backward pass.\n",
    "  - 4-bit weights + gradient checkpointing reduce compute per forward pass.\n",
    "  - Small batch size / sequence length also limit utilization.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Strategies to Increase GPU Utilization\n",
    "\n",
    "1. **Increase batch size**\n",
    "   - Current: 8 × 4 = 32 effective batch\n",
    "   - Can go higher if VRAM allows → fills GPU cores better.\n",
    "\n",
    "2. **Increase sequence length**\n",
    "   - Current: `MAX_LENGTH = 512`\n",
    "   - Longer sequences increase compute per batch → higher utilization.\n",
    "\n",
    "3. **Train more LoRA layers**\n",
    "   - Currently only `q_proj` and `v_proj` in attention.\n",
    "   - Including MLP layers or `k_proj`/`o_proj` increases trainable parameters → more GPU compute.\n",
    "\n",
    "4. **Optimize data loading**\n",
    "   - `dataloader_num_workers` = 32\n",
    "   - Pin memory = True\n",
    "   - Ensures GPU is not idle waiting for CPU.\n",
    "\n",
    "5. **Multi-GPU (optional)**\n",
    "   - Spreading the model across multiple GPUs boosts compute usage.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Why Q and V Matrices in Attention?\n",
    "\n",
    "- **Attention mechanism:**\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d}}\\right) V\n",
    "\\]\n",
    "\n",
    "- **Q (Query)** → determines *which tokens to focus on*\n",
    "- **V (Value)** → determines *what information is propagated*\n",
    "\n",
    "**Choosing Q+V for LoRA:**\n",
    "\n",
    "- High leverage: small adapters in these matrices produce **significant task-specific adaptation**.\n",
    "- Minimal parameters needed → efficient training.\n",
    "- K and O, or MLP layers, can be adapted later for more aggressive fine-tuning, but Q+V gives most effect per parameter.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. How LoRA Works (Simplified)\n",
    "\n",
    "1. Identify target matrices in the model (Q and V projections).\n",
    "2. Freeze original weights `W`.\n",
    "3. Add **low-rank matrices** `A` and `B` such that `ΔW = BA`.\n",
    "4. Train `BA` only, keeping all other parameters frozen.\n",
    "5. During forward pass:\n",
    "   - `W' = W + BA` is used in attention computations\n",
    "   - Only `BA` gradients are computed → lightweight backward pass\n",
    "\n",
    "**Effect:** The model “learns” new behavior efficiently, without touching billions of frozen parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "| Feature                  | Choice / Value                        | Reason                                                                 |\n",
    "|---------------------------|--------------------------------------|------------------------------------------------------------------------|\n",
    "| Model                     | Mistral-7B-Instruct-v0.3             | Large LLM, strong instruction-following capabilities                 |\n",
    "| Fine-tuning method        | LoRA                                  | Parameter-efficient, memory-friendly                                   |\n",
    "| Quantization              | 4-bit NF4                             | Reduce VRAM, maintain speed                                           |\n",
    "| Target LoRA layers        | `q_proj` + `v_proj`                   | Max effect on attention with minimal parameters                        |\n",
    "| Batch size                | 8 × 4 = 32 effective                  | Fits GPU memory, can be increased for better utilization               |\n",
    "| Gradient checkpointing    | Enabled                               | Saves memory during training                                           |\n",
    "| GPU utilization           | ~40%, 4–5GB / 48GB                    | Expected for tiny trainable adapter on frozen 7B model                 |\n",
    "| Improvements to utilization | longer sequences, more LoRA layers, larger batches | More GPU compute, still memory safe                                  |\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**\n",
    "\n",
    "- **LoRA + 4-bit quantization** allows fine-tuning **large models efficiently**.\n",
    "- Targeting **attention Q+V matrices** gives maximal effect for minimal compute.\n",
    "- GPU underutilization is normal due to **tiny trainable portion**; utilization can be increased with **larger sequences, batch size, or additional adapters**.\n",
    "\n"
   ],
   "id": "470d02069181a1ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "80ae922cd19c15fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
