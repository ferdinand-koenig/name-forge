| Model Version | Main Changes / Fine-tuning       | Results / Observations                                                                        | Dataset Used | Notes                                                                                                                                                    |
|---------------|----------------------------------|-----------------------------------------------------------------------------------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
| v. 1.0        | Default LoRA (3 epochs, 2e-4 LR) | High repetition of domain suffixes; outputs often repeated same domains                       | v. 1.0       |                                                                                                                                                          |
| v. 2.0        | Use new data set                 | Slightly better diversity; repetition still present but fewer duplicates. Often not creative. | v. 2.0       |                                                                                                                                                          |
| v. 2.1        | 4 epochs (1.7e-4 LR)             | Significantly improved diversity; valid JSON arrays; minimal repetition                       | v. 2.0       | Longer training helps LoRA adapters generalize; moderate improvement. Still same training time due to `gradient_checkpointing=False,` but uses more VRAM |




Dataset part `-a` is generated by data generator (`src/data_utils.py`), part `-b` is generated by ChatGPT.
See report.
